{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerekli Kütüphanelerin Eklenmesi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerekli kütüphaneleri projeye ekleyerek başlıyoruz. \n",
    "import pandas as pd #veri işlemleri için pandas kütüphanesini kullanırız\n",
    "import numpy as np  #sayısal işlemler için numpy kullanacağız\n",
    "import matplotlib.pyplot as plt #grafik işlemleri için \n",
    "from sklearn.model_selection import train_test_split #veri bölme\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler #veri ölçeklendirme\n",
    "\n",
    "from sklearn.svm import SVR #SVR modeli ---> benim ödevdeki algoritmam \n",
    "from sklearn.linear_model import LinearRegression #lineer regresyon \n",
    "from sklearn.preprocessing import PolynomialFeatures #Polinom regresyon \n",
    "from sklearn.metrics import mean_absolute_error, r2_score #Değerlendirme metrikleri\n",
    "\n",
    "#grafiklerin notebook içinde görünmesini sağlamak için aşağıdaki komutu yazıyoruz\n",
    "%matplotlib inline \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Önişleme "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Bir ihtimal veri url yolu ile okunması gerekirse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00547/Algerian_forest_fires_dataset_UPDATE.csv\"\n",
    "#df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veri Setini Okuma Ve İlk 5 Veriyi Yazdırma / Eksik Değerleri Kontrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verileri okuyacağız \n",
    "file_path = \"data/forest_fires.csv\" #veri setinin yolunu yazıyoruz. \n",
    "data = pd.read_csv(file_path, skiprows=1, header=0, encoding='utf-8') #Burada dosyayı okuyoruz. \n",
    "print(\"Veri Setinin Ilk 5 Satırı:\") #veri setinde bulunan ilk 5 veriyi çekiyoruz\n",
    "display(data.head()) #ilk 5 veriyi çekecek komut\n",
    "\n",
    "print(\"\\nVeri Setinin Bilgisi:\")\n",
    "data.info() #veri seti hakkında genel bilgiyi bize yazacak\n",
    "\n",
    "print(\"\\nEksik Değerler:\")\n",
    "print(data.isnull().sum()) #eksik değerleri kontol edeceğiz ve print ile yazdıracağız\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eksik Değerlerin Temizlenmesi/Düzeltilmesi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ilk denemem de veri türlerini dikkate almadığım için hata aldım\n",
    "#veri türlerini kontrol ederek sütunları inceleyelim \n",
    "print(\"\\n Sütunlar ve veri türleri\")\n",
    "print(data.dtypes) #bu komut ile veri türlerini kontrol ediyoruz :)\n",
    "\n",
    "#şimdi eksik değerleri ortalama ile dolduracağımız için sadece sayısal değerleri alıyoruz\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns \n",
    "\n",
    "#eksik sütunları ortalama alarak tamamlıyoruz\n",
    "print(\"\\n Eksik değerlerin doldurulması\")\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
    "\n",
    "#Eksik değerler doldurulduktan sonrası\n",
    "print(\"\\n Sonrası\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\n İlk 5 veri\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Öznitelik Seçimi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ödev de bana \"ilk 1-6 arası öznitelikler kullanılarak modelleme yapılacaktır \" şeklinde bir ibare verildiği için buna dikkate alarak öznitelik seçimlerini yapacağım\n",
    "# ilk 1-6 sütun bağımsız değişkenler yani x olacak \n",
    "x = data.iloc[:, 1:7]\n",
    "\n",
    "#Bağımlı değişken olarak FWI sütununu seçeceğim\n",
    "y = data['FWI']\n",
    "\n",
    "#Seçimleri doğrulamak için verilerin boyutunu kontrol edeceğiz\n",
    "print(\"\\n Bağımsız Değişken Boyutu\", x.shape)\n",
    "print(\"\\n Bağımlı Değişken Boyutu\", y.shape )\n",
    "\n",
    "#ilk 5 satır görüntüleme \n",
    "print(\"\\n Bağımsız Değişkenler:\")\n",
    "display(x.head())\n",
    "print(\"\\n Bağımlı Değişken:\")\n",
    "display(y.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verinin Eğitim ve Test Setlerine Ayrılması "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veri setini eğitim ve test verisi olarak ikiye böleceğiz. \n",
    "#Ben %70 eğitim - %30 test verisi olarak ayıracağım fakat %80 eğitim - %20 test şeklinde de bölünebilir\n",
    "#Ben projenin performansını ölçmek için 70-30 seçtim. Model daha iyi öğrensin istersek 80-20 seçilebilir.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=42) #bu komut ile veri setini böldük\n",
    "print(\"\\nEğitim Seti Boyutu:\")\n",
    "print(\"x_train:\", x_train.shape, \"y_train:\", y_train.shape) #shape ile görüntüleme işlemi yapacağız\n",
    "print(\"\\nTest Seti Boyutu:\")\n",
    "print(\"x_test:\", x_test.shape, \"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veri Ölçeklendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ölçeklendirme Eğitim Seti Boyutu (172, 6)\n",
      "\n",
      " Ölçeklendirilmiş Test Seti Boyutu: (74, 6)\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler, veri setindeki özelliklerin(features) standartlaştırılmasını sağlar. \n",
    "#sklearn.preprocessing modülünde yer alır\n",
    "#veriyi hazırlamak için kullanacağız \n",
    "#Bunu kullanmak bize 2 katkı sağlayacak\n",
    "#1. Modelin daha hızlı ve daha doğru öğrenmesi sağlanır.\n",
    "#2. Özniteliklerin (features) farklı ölçekleri arasında dengesizlik önlenir.\n",
    "\n",
    "#SVR modelinde veri ölçeklendirme yapacağız. Bunu yaparken StandardScaler kullanarak normalleştirme yapacağız.\n",
    "#ilk yazdığım kodda sayısal olmayan değişkenleri hesaba katmamıştım. \n",
    "#StandardScaler yalnızca sayısal değerler üzerinde çalışır.\n",
    "\n",
    "#Sayısal sütunlara dönüştürme işlemi yapalım sonrasında NaN değerler için kontrol ve doldurma işlemi yapalım\n",
    "x_train = x_train.apply(pd.to_numeric, errors='coerce')\n",
    "x_test = x_test.apply(pd.to_numeric, errors='coerce')\n",
    "x_train = x_train.fillna(x_train.mean())#fillna() fonksiyonu ile eksik değerler doldurulur\n",
    "x_test = x_test.fillna(x_test.mean())\n",
    "\n",
    "scaler = StandardScaler() #Ölçeklendirme işlemine burada başlıyoruz\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train) #eğitim seti ölçeklendirme\n",
    "x_test_scaled = scaler.transform(x_test) #test setini ölçeklendirme işlemi\n",
    "\n",
    "print(\"\\n Ölçeklendirme Eğitim Seti Boyutu\", x_train_scaled.shape)\n",
    "print(\"\\n Ölçeklendirilmiş Test Seti Boyutu:\", x_test_scaled.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
